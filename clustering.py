# -*- coding: utf-8 -*-
"""Clustering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_x9GDQIQGS8fjAqmPdyVZKyEnGEhVBhZ

##Elbow Method
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from google.colab import drive
import os
import shutil
from tqdm import tqdm
from google.colab import drive
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from sklearn.preprocessing import LabelEncoder

# Step 2: Set up path to load extracted features
features_path = '/content/drive/MyDrive/extracted_features_realData/extracted_features.npy'

# Step 3: Load the extracted features
features = np.load(features_path)
print(f"Features shape: {features.shape}")  # Should be (19924, feature_size)

# Step 4: Use the Elbow Method to find the optimal number of clusters
wcss = []  # Within-cluster sum of squares
# Try k-means clustering for different values of k (e.g., 1 to 15 clusters)
for k in range(1, 16):

  kmeans = KMeans(n_clusters=k, random_state=42)
  kmeans.fit(features)
  wcss.append(kmeans.inertia_)  # Inertia is the sum of squared distances of samples to their closest cluster center

# Step 5: Plot the results to visualize the "elbow"
plt.figure(figsize=(8, 5))
plt.plot(range(1, 16), wcss, marker='o')
plt.title('Elbow Method for Optimal Number of Clusters')
plt.xlabel('Number of clusters (k)')
plt.ylabel('WCSS (Within-Cluster Sum of Squares)')
plt.show()

"""## Clustering"""

# Step : Set up paths
image_dir = '/content/drive/MyDrive/realdata-images'  # Path to original images
save_dir = '/content/drive/MyDrive/realData-clustered-folders'  # Path to save clustered images

num_clusters = 7
kmeans = KMeans(n_clusters=num_clusters, random_state=42)

##Step 5: Perform k-means clustering on the features
print("Performing k-means clustering...")
clusters = kmeans.fit_predict(features)
print("Clustering completed.")

# First, create directories for each cluster
for cluster_idx in range(num_clusters):
    cluster_folder = os.path.join(save_dir, f'cluster_{cluster_idx}')
    os.makedirs(cluster_folder, exist_ok=True)

# Step 7: Move images to their respective cluster folders
# Get the list of image paths
image_paths = []
for root, dirs, files in os.walk(image_dir):
    for file in files:
        if file.endswith(('.jpg', '.jpeg', '.png')):
            image_paths.append(os.path.join(root, file))

# Check if the number of image paths matches the number of clusters
assert len(image_paths) == len(clusters), "Number of images and cluster assignments should match."

for img_path, cluster_label in tqdm(zip(image_paths, clusters), total=len(image_paths)):
    # Determine the destination folder for this image
    cluster_folder = os.path.join(save_dir, f'cluster_{cluster_label}')

    # Copy the image to the corresponding cluster folder
    shutil.copy(img_path, cluster_folder)

print("Images have been saved to their respective cluster folders.")

"""## Check no of images into each cluster"""

# Path to the directory where cluster folders are stored
cluster_base_dir = '/content/drive/MyDrive/realData-clustered-folders'

# Get the list of cluster folders
cluster_folders = [f for f in os.listdir(cluster_base_dir) if os.path.isdir(os.path.join(cluster_base_dir, f))]

# Print the number of images in each cluster folder
for cluster_folder in sorted(cluster_folders):
    cluster_path = os.path.join(cluster_base_dir, cluster_folder)
    num_images = len([f for f in os.listdir(cluster_path) if f.endswith(('.png', '.jpg', '.jpeg'))])
    print(f'Cluster {cluster_folder}: {num_images} images')

"""## Divide Data using Train Test Split Approach

---


"""

# Paths
source_dir = '/content/drive/MyDrive/all-clustered-folders'  # Folder containing the images (clustered)
train_dir = '/content/drive/MyDrive/org_split_data/train'
val_dir = '/content/drive/MyDrive/org_split_data/validation'
test_dir = '/content/drive/MyDrive/org_split_data/test'

# Create directories for train, validation, and test sets
os.makedirs(train_dir, exist_ok=True)
os.makedirs(val_dir, exist_ok=True)
os.makedirs(test_dir, exist_ok=True)

# List all cluster folders
cluster_folders = [f for f in os.listdir(source_dir) if os.path.isdir(os.path.join(source_dir, f))]

# Loop over each cluster folder
for cluster_folder in cluster_folders:
    cluster_path = os.path.join(source_dir, cluster_folder)
    images = [f for f in os.listdir(cluster_path) if f.endswith(('.png', '.jpg', '.jpeg'))]

    # Split the images into train, test, and validation sets
    train_images, temp_images = train_test_split(images, test_size=0.30, random_state=42)  # 70% train
    val_images, test_images = train_test_split(temp_images, test_size=2/3, random_state=42)  # 10% val, 20% test

    # Create cluster subdirectories in train, validation, and test folders
    os.makedirs(os.path.join(train_dir, cluster_folder), exist_ok=True)
    os.makedirs(os.path.join(val_dir, cluster_folder), exist_ok=True)
    os.makedirs(os.path.join(test_dir, cluster_folder), exist_ok=True)

    # Move the images to their respective directories
    for image in train_images:
        shutil.move(os.path.join(cluster_path, image), os.path.join(train_dir, cluster_folder, image))

    for image in val_images:
        shutil.move(os.path.join(cluster_path, image), os.path.join(val_dir, cluster_folder, image))

    for image in test_images:
        shutil.move(os.path.join(cluster_path, image), os.path.join(test_dir, cluster_folder, image))

    print(f"Cluster {cluster_folder}: Train {len(train_images)}, Validation {len(val_images)}, Test {len(test_images)}")

print("Data split complete.")

"""## Convert Data into Numpy File"""

# Paths to folders containing the original images
traditional_augmented_data = '/content/drive/MyDrive/realData-clustered-folders'
# val_dir = '/content/drive/MyDrive/org_split_data/validation'
# test_dir = '/content/drive/MyDrive/org_split_data/test'

# Path to the folder where you want to save the .npy files
save_npy_dir = '/content/drive/MyDrive/npy-clustered-realData'

# Image size parameters (ViT requires 224x224 size images)
IMG_SIZE = (224, 224)

def convert_images_and_labels_to_npy(input_folder, label_encoder):
    images = []
    labels = []

    for subfolder in os.listdir(input_folder):
        subfolder_path = os.path.join(input_folder, subfolder)
        if os.path.isdir(subfolder_path):
            label = subfolder
            label_index = label_encoder.transform([label])[0]

            # Loop over all images in the subfolder
            for filename in os.listdir(subfolder_path):
                file_path = os.path.join(subfolder_path, filename)

                # Check if it's an image file
                if filename.endswith(('.png', '.jpg', '.jpeg')):
                    try:
                        # Load the image and convert it to an array
                        image = load_img(file_path, target_size=IMG_SIZE)
                        image_array = img_to_array(image)

                        # Append image and label to lists
                        images.append(image_array)
                        labels.append(label_index)
                    except Exception as e:
                        print(f"Error processing file {file_path}: {e}")

    # Convert lists to numpy arrays
    images_np = np.array(images)
    labels_np = np.array(labels)

    return images_np, labels_np

# Collect all labels for encoding
all_labels = set()
for folder in [traditional_augmented_data]:
    for subfolder in os.listdir(folder):
        if os.path.isdir(os.path.join(folder, subfolder)):
            all_labels.add(subfolder)

# Encode labels
label_encoder = LabelEncoder()
label_encoder.fit(list(all_labels))

# Ensure the save directory exists
os.makedirs(save_npy_dir, exist_ok=True)

# Convert and save data
for dataset_name, folder in [('npy_clautered_data', traditional_augmented_data)]:
    try:
        images_np, labels_np = convert_images_and_labels_to_npy(folder, label_encoder)

        # Save images and labels to .npy files
        images_npy_path = os.path.join(save_npy_dir, f'{dataset_name}_images.npy')
        labels_npy_path = os.path.join(save_npy_dir, f'{dataset_name}_labels.npy')

        np.save(images_npy_path, images_np)
        np.save(labels_npy_path, labels_np)

        print(f"Saved {images_npy_path}")
        print(f"Saved {labels_npy_path}")

    except Exception as e:
        print(f"Error processing dataset {dataset_name}: {e}")

print("All images and labels have been converted to .npy format and saved.")

# Paths to folders containing the original images
train_dir = '/content/drive/MyDrive/org_split_data/train'
val_dir = '/content/drive/MyDrive/org_split_data/validation'
test_dir = '/content/drive/MyDrive/org_split_data/test'

# Path to the folder where you want to save the .npy files
save_npy_dir = '/content/drive/MyDrive/balance_data_npy/'

# Image size parameters (ViT requires 224x224 size images)
IMG_SIZE = (224, 224)

def convert_images_and_labels_to_npy(input_folder, label_encoder):
    images = []
    labels = []

    for subfolder in os.listdir(input_folder):
        subfolder_path = os.path.join(input_folder, subfolder)
        if os.path.isdir(subfolder_path):
            label = subfolder
            label_index = label_encoder.transform([label])[0]

            # Loop over all images in the subfolder
            for filename in os.listdir(subfolder_path):
                file_path = os.path.join(subfolder_path, filename)

                # Check if it's an image file
                if filename.endswith(('.png', '.jpg', '.jpeg')):
                    try:
                        # Load the image and convert it to an array
                        image = load_img(file_path, target_size=IMG_SIZE)
                        image_array = img_to_array(image)

                        # Append image and label to lists
                        images.append(image_array)
                        labels.append(label_index)
                    except Exception as e:
                        print(f"Error processing file {file_path}: {e}")

    # Convert lists to numpy arrays
    images_np = np.array(images)
    labels_np = np.array(labels)

    return images_np, labels_np

# Collect all labels for encoding
all_labels = set()
for folder in [train_dir, val_dir, test_dir]:
    for subfolder in os.listdir(folder):
        if os.path.isdir(os.path.join(folder, subfolder)):
            all_labels.add(subfolder)

# Encode labels
label_encoder = LabelEncoder()
label_encoder.fit(list(all_labels))

# Ensure the save directory exists
os.makedirs(save_npy_dir, exist_ok=True)

# Convert and save data
for dataset_name, folder in [('train_dir', train_dir), ('val_dir', val_dir), ('test_dir', test_dir)]:
# for dataset_name, folder in [('train_dir', train_dir, 'val_dir', val_dir, 'test_dir', test_dir)]:
    try:
        images_np, labels_np = convert_images_and_labels_to_npy(folder, label_encoder)

        # Save images and labels to .npy files
        images_npy_path = os.path.join(save_npy_dir, f'{dataset_name}_images.npy')
        labels_npy_path = os.path.join(save_npy_dir, f'{dataset_name}_labels.npy')

        np.save(images_npy_path, images_np)
        np.save(labels_npy_path, labels_np)

        print(f"Saved {images_npy_path}")
        print(f"Saved {labels_npy_path}")

    except Exception as e:
        print(f"Error processing dataset {dataset_name}: {e}")

print("All images and labels have been converted to .npy format and saved.")

