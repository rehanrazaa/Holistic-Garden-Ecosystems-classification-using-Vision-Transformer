# -*- coding: utf-8 -*-
"""Proposed_VisionNet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11GX3-msfINgpbIwPmXuRm1Efbu3d2JNn
"""

!pip install torch torchvision timm

"""## Import Library"""

import os
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import timm
from sklearn.model_selection import train_test_split
from torchvision import transforms
import matplotlib.pyplot as plt

"""## Load Data"""

# Paths to .npy files
train_images_path = '/content/drive/MyDrive/balance_data_npy/train_dir_images.npy'
train_labels_path = '/content/drive/MyDrive/balance_data_npy/train_dir_labels.npy'
val_images_path = '/content/drive/MyDrive/balance_data_npy/val_dir_images.npy'
val_labels_path = '/content/drive/MyDrive/balance_data_npy/val_dir_labels.npy'
test_images_path = '/content/drive/MyDrive/balance_data_npy/test_dir_images.npy'
test_labels_path = '/content/drive/MyDrive/balance_data_npy/test_dir_labels.npy'
aug_images_path = '/content/drive/MyDrive/traditional_augmented_data_npy/traditional_augmented_data_images.npy'
aug_labels_path = '/content/drive/MyDrive/traditional_augmented_data_npy/traditional_augmented_data_labels.npy'

# Load .npy files
train_images = np.load(train_images_path)
train_labels = np.load(train_labels_path)
val_images = np.load(val_images_path)
val_labels = np.load(val_labels_path)
test_images = np.load(test_images_path)
test_labels = np.load(test_labels_path)
aug_images = np.load(aug_images_path)
aug_labels = np.load(aug_labels_path)

# Normalize the images (ViT requires normalization to ImageNet stats)
train_images = train_images / 255.0
val_images = val_images / 255.0
test_images = test_images / 255.0
aug_images = aug_images / 255.0


# Resize images to (224, 224) as ViT requires 224x224 input
IMG_SIZE = (224, 224)

#Confirming Shape

print(train_images.shape)
print(train_labels.shape)
print(val_images.shape)
print(val_labels.shape)
print(test_images.shape)
print(test_labels.shape)
print(aug_images.shape)
print(aug_labels.shape)

X_data = np.concatenate([train_images,val_images,test_images,aug_images])
X_labels = np.concatenate([train_labels,val_labels,test_labels,aug_labels])

"""# Dividing the Data Using Train Test Split"""

X_Train,X_test,Y_Train,Y_test=train_test_split(X_data,X_labels,test_size=0.2,random_state = 3, shuffle=True)

print("x train=",X_Train.shape)
print("y train=",Y_Train.shape)

print("x test=",X_test.shape)
print("y test=",Y_test.shape)

from sklearn.model_selection import train_test_split
X_train,X_val,Y_train,Y_val=train_test_split(X_Train,Y_Train,test_size=0.1,random_state = 3, shuffle=True)

print("x train=",X_train.shape)
print("y train=",Y_train.shape)

print("x valid=",X_val.shape)
print("y valid=",Y_val.shape)

# Transformations for the images
transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize(IMG_SIZE),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization
])

# Create a custom dataset class
class GardenDataset(Dataset):
    def __init__(self, images, labels, transform=None):
        self.images = images
        self.labels = labels
        self.transform = transform

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        image = self.images[idx]
        label = self.labels[idx]

        if self.transform:
            image = self.transform(image)

        return image, label

# Create train, validation, and test datasets
train_dataset = GardenDataset(X_train, Y_train, transform=transform)
val_dataset = GardenDataset(X_val, Y_val, transform=transform)
test_dataset = GardenDataset(X_test, Y_test, transform=transform)

# DataLoader for batching
BATCH_SIZE = 16

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

"""## Load Model Vit"""

# Build the Vision Transformer (ViT) model using timm
model = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=7)

# Freeze all layers
for param in model.parameters():
    param.requires_grad = False

# Unfreeze the last few layers (The last transformer block and classification head)


# Unfreeze the classification head
for param in model.head.parameters():
    param.requires_grad = True

# Unfreeze the last transformer block
for param in model.blocks[-1].parameters():
    param.requires_grad = True

# Move the model to GPU if available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

# Define loss and optimizer
criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-3)

# Training loop
EPOCHS = 200

scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)  # Reduces LR by 0.1 every 5 epochs

import torch
print(torch.cuda.is_available())  # Returns True if GPU is available

"""## Train Model"""

def train_model(model, train_loader, val_loader, criterion, optimizer, epochs):
    for epoch in range(epochs):
        model.train()
        running_loss = 0.0
        correct_train = 0
        total_train = 0

        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)

            optimizer.zero_grad()

            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

            _, predicted = outputs.max(1)
            total_train += labels.size(0)
            correct_train += predicted.eq(labels).sum().item()

        train_acc = correct_train / total_train
        print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}, Accuracy: {train_acc:.4f}')

        # Validation
        model.eval()
        correct_val = 0
        total_val = 0
        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)

                _, predicted = outputs.max(1)
                total_val += labels.size(0)
                correct_val += predicted.eq(labels).sum().item()

        val_acc = correct_val / total_val
        print(f'Validation Accuracy: {val_acc:.4f}')
        # Save the model at the end of each epoch
        torch.save(model.state_dict(), f'/content/drive/MyDrive/VisionNet_v3_balanceData_models/Tl_vit_v3_epoch_{epoch+1}.pth')
        print(f"Model saved successfully at epoch {epoch+1}!")

        # Learning rate scheduler step
        if scheduler:
            scheduler.step()

train_model(model, train_loader, val_loader, criterion, optimizer, EPOCHS)

# Resume Training Process
checkpoint_path = '/content/drive/MyDrive/VisionNet_v3_balanceData_models/Tl_vit_v3_epoch_33.pth'
model.load_state_dict(torch.load(checkpoint_path))
model = model.to(device)

# Define loss and optimizer with potentially a lower learning rate for fine-tuning
# Reduce learning rate to fine-tune the model
new_learning_rate = 1e-5
optimizer = optim.AdamW(model.parameters(), lr=new_learning_rate, weight_decay=1e-3)

# Training loop (continued training)
def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, start_epoch, total_epochs):
    for epoch in range(start_epoch, total_epochs):
        model.train()
        running_loss = 0.0
        correct_train = 0
        total_train = 0

        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)

            optimizer.zero_grad()

            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

            _, predicted = outputs.max(1)
            total_train += labels.size(0)
            correct_train += predicted.eq(labels).sum().item()

        train_acc = correct_train / total_train
        print(f'Epoch {epoch+1}/{total_epochs}, Loss: {running_loss/len(train_loader):.4f}, Accuracy: {train_acc:.4f}')

        # Validation
        model.eval()
        correct_val = 0
        total_val = 0
        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)

                _, predicted = outputs.max(1)
                total_val += labels.size(0)
                correct_val += predicted.eq(labels).sum().item()

        val_acc = correct_val / total_val
        print(f'Validation Accuracy: {val_acc:.4f}')

        # Save the model at the end of each epoch
        torch.save(model.state_dict(), f'/content/drive/MyDrive/VisionNet_v3_balanceData_models/Tl_vit_v3_epoch_{epoch+1}.pth')
        print(f"Model saved successfully at epoch {epoch+1}!")

        # Step the scheduler
        if scheduler:
            scheduler.step()

# Set the scheduler if needed, continue from a reduced step_size and gamma
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)

# Start training from epoch 25
start_epoch = 33
total_epochs = 150
train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, start_epoch, total_epochs)

"""## Test model"""

# Load the saved model
model.load_state_dict(torch.load('/content/drive/MyDrive/VisionNet_v3_balanceData_models/Tl_vit_v3_epoch_9.pth'))

# Test the model on the test set
def test_model(model, test_loader):
    model.eval()
    correct_test = 0
    total_test = 0
    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)

            _, predicted = outputs.max(1)
            total_test += labels.size(0)
            correct_test += predicted.eq(labels).sum().item()

    test_acc = correct_test / total_test
    print(f'Test Accuracy: {test_acc:.4f}')

# Evaluate the model on test data
test_model(model, test_loader)

pip install matplotlib seaborn scikit-learn

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Test the model on the test set
def test_model(model, test_loader):
    model.eval()
    all_preds = []
    all_labels = []
    correct_test = 0
    total_test = 0

    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)

            _, predicted = outputs.max(1)
            all_preds.extend(predicted.cpu().numpy())  # Collect predictions
            all_labels.extend(labels.cpu().numpy())    # Collect true labels

            total_test += labels.size(0)
            correct_test += predicted.eq(labels).sum().item()

    test_acc = correct_test / total_test
    print(f'Test Accuracy: {test_acc:.4f}')

    return all_labels, all_preds  # Return collected labels and predictions

# Evaluate the model on test data
true_labels, predictions = test_model(model, test_loader)

"""## Confusion Matrix"""

def plot_confusion_matrix(true_labels, predictions, classes):
    cm = confusion_matrix(true_labels, predictions)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)

    # Increase the figure size
    plt.figure(figsize=(10, 8))  # Adjust the width and height as needed
    disp.plot(cmap=plt.cm.Blues)

    plt.title('Confusion Matrix')
    plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better visibility
    plt.tight_layout()  # Adjust layout to make room for labels
    plt.show()

# Assuming you have a list of class names
class_names = ['aquatic_plants', 'clustered_flowers', 'floral_garden', 'fruits_vegetables', 'greenhouse_garden', 'wildflowers', 'woodland_trees']
plot_confusion_matrix(true_labels, predictions, class_names)

"""## Classification Report, Auc-Roc-Curve, Training Validation Loss Curve"""

import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import numpy as np
from torchvision import models
from sklearn.metrics import roc_auc_score, roc_curve, classification_report, confusion_matrix, ConfusionMatrixDisplay
from sklearn.preprocessing import label_binarize
from itertools import cycle

# Define your model, loaders, and training loop here

# Step 1: Plot the ROC curve (for multi-class)
# def plot_auc_roc_curve(true_labels, model, test_loader, num_classes, class_names):
#     true_labels_binarized = label_binarize(true_labels, classes=list(range(num_classes)))
#     all_probs = []
#     model.eval()
#     with torch.no_grad():
#         for images, labels in test_loader:
#             images = images.to(device)
#             outputs = model(images)
#             probs = torch.softmax(outputs, dim=1)
#             all_probs.extend(probs.cpu().numpy())

#     fpr = dict()
#     tpr = dict()
#     roc_auc = dict()
#     for i in range(num_classes):
#         fpr[i], tpr[i], _ = roc_curve(true_labels_binarized[:, i], np.array(all_probs)[:, i])
#         roc_auc[i] = roc_auc_score(true_labels_binarized[:, i], np.array(all_probs)[:, i])

#     plt.figure(figsize=(10, 8))
#     colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])
#     for i, color in zip(range(num_classes), colors):
#         plt.plot(fpr[i], tpr[i], color=color, lw=2,
#                  label=f'ROC curve of class {i} (area = {roc_auc[i]:0.2f})')

#     plt.plot([0, 1], [0, 1], 'k--', lw=2)
#     plt.xlim([0.0, 1.0])
#     plt.ylim([0.0, 1.05])
#     plt.xlabel('False Positive Rate')
#     plt.ylabel('True Positive Rate')
#     plt.title('Receiver Operating Characteristic (ROC)')
#     plt.legend(loc="lower right")
#     plt.show()

# Step 2: Classification Report
def generate_classification_report(true_labels, predictions, class_names):
    print(classification_report(true_labels, predictions, target_names=class_names))

# Step 3: Plot Training and Validation Curves
def plot_training_curves(train_losses, val_losses, train_accuracies, val_accuracies):
    epochs = range(1, len(train_losses) + 1)

    plt.figure(figsize=(12, 5))

    # Plot Loss
    plt.subplot(1, 2, 1)
    plt.plot(epochs, train_losses, label='Training Loss')
    plt.plot(epochs, val_losses, label='Validation Loss')
    plt.title('Loss Curve')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    # Plot Accuracy
    plt.subplot(1, 2, 2)
    plt.plot(epochs, train_accuracies, label='Training Accuracy')
    plt.plot(epochs, val_accuracies, label='Validation Accuracy')
    plt.title('Accuracy Curve')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.tight_layout()
    plt.show()

# Step 4: Confusion Matrix (Already provided in your code)

def plot_auc_roc_curve(true_labels, model, test_loader, num_classes, class_names):
    true_labels_binarized = label_binarize(true_labels, classes=list(range(num_classes)))
    all_probs = []
    model.eval()
    with torch.no_grad():
        for images, labels in test_loader:
            images = images.to(device)
            outputs = model(images)
            probs = torch.softmax(outputs, dim=1)
            all_probs.extend(probs.cpu().numpy())

    fpr = dict()
    tpr = dict()
    roc_auc = dict()
    for i in range(num_classes):
        fpr[i], tpr[i], _ = roc_curve(true_labels_binarized[:, i], np.array(all_probs)[:, i])
        roc_auc[i] = roc_auc_score(true_labels_binarized[:, i], np.array(all_probs)[:, i])

    plt.figure(figsize=(10, 8))
    colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green', 'red', 'purple', 'brown'])  # Add more colors if needed
    for i, color in zip(range(num_classes), colors):
        plt.plot(fpr[i], tpr[i], color=color, lw=2,
                 label=f'ROC curve of {class_names[i]} (area = {roc_auc[i]:0.2f})')

    plt.plot([0, 1], [0, 1], 'k--', lw=2)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC)')
    plt.legend(loc="lower right")
    plt.show()

# After testing the model
true_labels, predictions = test_model(model, test_loader)

# AUC-ROC curve
NUM_CLASSES = 7
class_names = ['aquatic_plants', 'clustered_flowers', 'floral_garden', 'fruits_vegetables', 'greenhouse_garden', 'wildflowers', 'woodland_trees']
# plot_confusion_matrix(true_labels, predictions, class_names)
plot_auc_roc_curve(true_labels, model, test_loader, NUM_CLASSES, class_names)

# Classification report
generate_classification_report(true_labels, predictions, class_names)

from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score

def compute_metrics(true_labels, predictions, class_names):
    # Generate classification report
    print("Classification Report:")
    print(classification_report(true_labels, predictions, target_names=class_names))

    # Compute Confusion Matrix
    cm = confusion_matrix(true_labels, predictions)
    print(f"Confusion Matrix:\n{cm}")

    # Calculate Precision, Recall, F1-Score for each class
    precision = precision_score(true_labels, predictions, average=None)
    recall = recall_score(true_labels, predictions, average=None)
    f1 = f1_score(true_labels, predictions, average=None)

    # Print the metrics
    print("\nPer-Class Metrics:")
    for idx, class_name in enumerate(class_names):
        print(f"Class: {class_name}")
        print(f"  Precision: {precision[idx]:.4f}")
        print(f"  Recall: {recall[idx]:.4f}")
        print(f"  F1-Score: {f1[idx]:.4f}")

    # Calculate Specificity for each class
    specificity = []
    for i in range(len(class_names)):
        true_negatives = cm.sum() - (cm[i, :].sum() + cm[:, i].sum() - cm[i, i])
        false_positives = cm[:, i].sum() - cm[i, i]
        specificity.append(true_negatives / (true_negatives + false_positives))

    # Print Specificity
    print("\nSpecificity for each class:")
    for idx, class_name in enumerate(class_names):
        print(f"Class: {class_name}")
        print(f"  Specificity: {specificity[idx]:.4f}")

# Call the function to compute and print metrics
compute_metrics(true_labels, predictions, class_names)

